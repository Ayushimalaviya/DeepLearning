{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oDOBSpmVpf14"
      },
      "source": [
        "## Logistic Regression.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rnp8IjhBpf16"
      },
      "source": [
        "#### Built 6 models and trained Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also, plot their objective values versus epochs and compare their training and testing accuracy and tuned the parameters a little bit to obtain reasonable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mNsq3Pqppf17"
      },
      "outputs": [],
      "source": [
        "# Load Packages\n",
        "import pandas as pd\n",
        "import numpy \n",
        "import decimal as dc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q24KGWTzpf17"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "- Download the Breast Cancer dataset from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
        "- Load the data.\n",
        "- Preprocess the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7AD-ZYa3pf17"
      },
      "source": [
        "## 1.1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__XOSHzEpf17",
        "outputId": "7c2cd617-b6c5-43b1-f337-c701942c81e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id                         0\n",
            "diagnosis                  0\n",
            "radius_mean                0\n",
            "texture_mean               0\n",
            "perimeter_mean             0\n",
            "area_mean                  0\n",
            "smoothness_mean            0\n",
            "compactness_mean           0\n",
            "concavity_mean             0\n",
            "concave points_mean        0\n",
            "symmetry_mean              0\n",
            "fractal_dimension_mean     0\n",
            "radius_se                  0\n",
            "texture_se                 0\n",
            "perimeter_se               0\n",
            "area_se                    0\n",
            "smoothness_se              0\n",
            "compactness_se             0\n",
            "concavity_se               0\n",
            "concave points_se          0\n",
            "symmetry_se                0\n",
            "fractal_dimension_se       0\n",
            "radius_worst               0\n",
            "texture_worst              0\n",
            "perimeter_worst            0\n",
            "area_worst                 0\n",
            "smoothness_worst           0\n",
            "compactness_worst          0\n",
            "concavity_worst            0\n",
            "concave points_worst       0\n",
            "symmetry_worst             0\n",
            "fractal_dimension_worst    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/breast-cancer-wisconsin.csv\", na_values = \"?\")\n",
        "print(data.isna().sum())  #missing values in data\n",
        "n = data.shape[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4-xZBlDGpf17"
      },
      "source": [
        "## 1.2 Examine and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kA_hdywBpf17"
      },
      "outputs": [],
      "source": [
        "# I have dropped id number as it is useless for achieving actual goal \n",
        "# I transformed target labels in the second column from 'B' and 'M' to 1 and -1.\n",
        "for i in range(len(data)):\n",
        "    if data.diagnosis.values[i] == 'B':\n",
        "        data.diagnosis.values[i] = 1\n",
        "    elif data.diagnosis.values[i] == 'M':\n",
        "        data.diagnosis.values[i] = -1\n",
        "X = data.iloc[:,2:32]\n",
        "y = data.iloc[:,1]\n",
        "X = numpy.concatenate((X, numpy.ones((n,1))), axis = 1)\n",
        "data1 = data.iloc[:,1:32]\n",
        "data1 = numpy.concatenate((data1, numpy.ones((n,1))), axis = 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6LmYL8L0pf17"
      },
      "source": [
        "## 1.3. Partition to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IwAYxpdpf17",
        "outputId": "4d5af913-77b7-44df-f4f0-ca30e7066247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(455, 31) (114, 31) (455,) (114,)\n"
          ]
        }
      ],
      "source": [
        "# I partitioned using 80% training data and 20% testing data. It is a commonly used ratio in machine learning.\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y ,test_size=0.20, random_state=104)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "exqYGRjhpf17"
      },
      "source": [
        "## 1.4. Feature scaling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mdt0pVNZpf18"
      },
      "source": [
        "Use the standardization to transform both training and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKu0haHpf18",
        "outputId": "ecc92450-11f6-4ffa-848f-de0be145f0c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test mean = \n",
            "[-0.03918771  0.03224115 -0.034305   -0.04762005  0.05548801  0.04032117\n",
            "  0.04294226  0.03454617  0.1057932   0.08823995 -0.02361691 -0.07529889\n",
            " -0.03524359 -0.04876564  0.08804058 -0.03433828 -0.00767958  0.05448303\n",
            " -0.1066496  -0.00529486 -0.01567734  0.0491125  -0.02178224 -0.01692623\n",
            "  0.12538072 -0.02797654  0.02510498  0.0411214  -0.01728766  0.07675517\n",
            "  0.        ]\n",
            "test std = \n",
            "[0.9073232  0.96373579 0.90015462 0.85759948 1.02631216 1.04825854\n",
            " 1.00339182 0.93860019 1.04465493 1.11160245 0.82606559 0.82066776\n",
            " 0.81033187 0.72510866 0.88613649 0.90586538 0.78215094 0.88572424\n",
            " 0.87157251 0.83118636 0.93290491 1.06839362 0.91486589 0.94036226\n",
            " 1.13386132 0.97824412 1.00014728 0.93624929 0.77181074 1.22789735\n",
            " 0.        ]\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "\n",
        "\n",
        "# calculate mu and sig using the training set\n",
        "d = x_train.shape[1]\n",
        "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
        "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
        "\n",
        "# transform the training features\n",
        "x_train = (x_train - mu) / (sig + 1E-6)\n",
        "\n",
        "# transform the test features\n",
        "x_test = (x_test - mu) / (sig + 1E-6)\n",
        "\n",
        "print('test mean = ')\n",
        "print(numpy.mean(x_test, axis=0))\n",
        "print('test std = ')\n",
        "print(numpy.std(x_test, axis=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z8CWAIxQpf18"
      },
      "source": [
        "# 2.  Logistic Regression Model\n",
        "\n",
        "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "When $\\lambda = 0$, the model is a regular logistic regression and when $\\lambda > 0$, it essentially becomes a regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JvJkIU3mpf18"
      },
      "outputs": [],
      "source": [
        "# Calculated the objective function value, or loss\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     objective function value, or loss (scalar)\n",
        "def objective(w, x, y, lam):\n",
        "    n = x.shape[0]\n",
        "    xy = numpy.multiply(x,y)\n",
        "    xyw = numpy.dot(xy,w)\n",
        "    val1 = numpy.array(xyw, dtype= numpy.float128)  #to avoid overflow issue\n",
        "\n",
        "    exp_term = (1 + numpy.exp(-val1)).astype(float)\n",
        "    term = numpy.log(exp_term)\n",
        "    log = 1/n * numpy.sum(term)\n",
        "    reg = 0.5 * lam * numpy.linalg.norm(w)**2\n",
        "    obj = log + reg\n",
        "    return obj"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1xxPP6eTpf18"
      },
      "source": [
        "# 3. Numerical optimization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xxvYStLBpf18"
      },
      "source": [
        "## 3.1. Gradient descent\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "45Od7gtYpf18"
      },
      "source": [
        "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AIReWGLxpf18"
      },
      "outputs": [],
      "source": [
        "# Calculated the gradient\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     g: gradient: d-by-1 matrix\n",
        "\n",
        "def gradient(w, x, y, lam):\n",
        "    n = x.shape[0]\n",
        "    xy = numpy.multiply(x,y)\n",
        "    xyw = numpy.dot(xy,w)\n",
        "    val1 = numpy.array(xyw, dtype= numpy.float128)\n",
        "    \n",
        "    exp_term = (1 + numpy.exp(val1)).astype(float)\n",
        "   # print(exp_term)\n",
        "    term = numpy.divide(xy,exp_term)\n",
        "    grad = -1/n * numpy.sum(term, axis=0).reshape(-1,1)   #summation of all rows for each colum to retrieve 31 by 1 matrix\n",
        "    reg = lam * w\n",
        "    g = grad + reg\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "D-p5pHQ4pf18"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for solving logistic regression\n",
        "# iterative processes (loops) were used to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "\n",
        "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    objvals = []\n",
        "    for epoch in range(max_epoch):\n",
        "        g = gradient(w, x, y, lam)\n",
        "        w = w - (learning_rate * g)\n",
        "        obj = objective(w,x,y,lam)\n",
        "        objvals.append(obj)\n",
        "    return w, objvals     #return update weights and last object values for each weights updated"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iagw_Mqpf18"
      },
      "source": [
        "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwzrazlepf18",
        "outputId": "6f2bd9f1-757c-4789-f795-d3a1887831f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal weights: [[-0.5235558452967161]\n",
            " [-0.5047581011781033]\n",
            " [-0.5124956557225834]\n",
            " [-0.5844880392682508]\n",
            " [-0.14142580780416708]\n",
            " [0.016099220991514288]\n",
            " [-0.563532857621189]\n",
            " [-0.643855678131429]\n",
            " [-0.11785508727476067]\n",
            " [0.2778683485772972]\n",
            " [-0.7566769753317775]\n",
            " [-0.11522812924321259]\n",
            " [-0.6139649998671523]\n",
            " [-0.6632216393616063]\n",
            " [-0.029076039600002174]\n",
            " [0.34230927136408545]\n",
            " [0.09814800059280165]\n",
            " [-0.022716267985073646]\n",
            " [0.11925969970549881]\n",
            " [0.3775075674656629]\n",
            " [-0.7550315535955442]\n",
            " [-0.7686831303640155]\n",
            " [-0.7108746542318009]\n",
            " [-0.775959051331699]\n",
            " [-0.4655012376023029]\n",
            " [-0.18604695566414933]\n",
            " [-0.5249205725829575]\n",
            " [-0.6560341346829839]\n",
            " [-0.47327857875736845]\n",
            " [-0.13391969058199263]\n",
            " [0.0]]\n",
            "objective value: [0.24062305041150986, 0.20739838968685337, 0.18616514204775222, 0.17126793734525889, 0.160174111996135, 0.1515352006914898, 0.14457065809618067, 0.13880305257808098, 0.1339251161090122, 0.12972969310054075, 0.1260714583638303, 0.12284503169837185, 0.119971866016535, 0.11739204584332852, 0.11505896046044944, 0.1129357303119703, 0.1109927419319089, 0.10920590604022443, 0.10755540045262016, 0.1060247458663901, 0.10460011506992488, 0.10326980893450106, 0.10202385359135353, 0.10085368700679732, 0.0997519124183284, 0.09871210240646511, 0.09772864175696257, 0.09679660035423676, 0.09591162955225704, 0.09506987706568157, 0.09426791659363658, 0.0935026892549594, 0.0927714545621926, 0.09207174915163986, 0.09140135186043313, 0.09075825402886495, 0.09014063412891375, 0.08954683599376338, 0.08897535005984065, 0.08842479714112308, 0.08789391434167751, 0.08738154278147243, 0.0868866168661748, 0.08640815487674526, 0.0859452506913736, 0.0854970664823509, 0.08506282625518344, 0.08464181011765562, 0.08423334918346957, 0.08383682102917461, 0.0834516456348763, 0.08307728174909235, 0.08271322362644065, 0.08235899809387127, 0.08201416190711076, 0.08167829936405172, 0.08135102014613964, 0.08103195736250521, 0.08072076577475837, 0.08041712018308816, 0.08012071395666262, 0.07983125769335682, 0.07954847799559812, 0.07927211635064947, 0.07900192810498352, 0.07873768152356443, 0.07847915692587124, 0.07822614589138764, 0.07797845052806739, 0.07773588279797168, 0.07749826389488376, 0.07726542366924093, 0.0770372000961997, 0.07681343878306973, 0.07659399251272553, 0.07637872081993738, 0.07616748959785774, 0.07596017073216405, 0.07575664176059364, 0.07555678555581775, 0.07536049002979, 0.07516764785787491, 0.07497815622121332, 0.07479191656591898, 0.07460883437782387, 0.07442881897160135, 0.07425178329319554, 0.07407764373457792, 0.07390631995993266, 0.07373773474244752, 0.07357181381095484, 0.07340848570572779, 0.07324768164279365, 0.07308933538617596, 0.07293338312752426, 0.07277976337263178, 0.07262841683437998, 0.07247928633168492, 0.07233231669405081, 0.07218745467136757]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# I obtained optimal weights and a list of objective values by using gradient_descent function.\n",
        "n, d = x_train.shape\n",
        "y_train = numpy.array(y_train).reshape(-1,1)\n",
        "\n",
        "lr = 0.5 \n",
        "lambdas = 0 \n",
        "\n",
        "# Initialize the weights to zero\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_gd, gd_obj = gradient_descent(x_train, y_train, lambdas, lr, w)\n",
        "# Print the optimal weights and the objective value\n",
        "print(\"Optimal weights:\", w_gd)\n",
        "print(\"objective value:\", gd_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-sCibklpf18",
        "outputId": "1b79bd9f-5cdf-4ed2-f5f0-46664d415371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal weights: [[-0.51258444]\n",
            " [-0.49311447]\n",
            " [-0.50181254]\n",
            " [-0.57222661]\n",
            " [-0.13795456]\n",
            " [ 0.01467999]\n",
            " [-0.55246313]\n",
            " [-0.6305754 ]\n",
            " [-0.11613254]\n",
            " [ 0.27077714]\n",
            " [-0.74009179]\n",
            " [-0.11162103]\n",
            " [-0.6006995 ]\n",
            " [-0.64865233]\n",
            " [-0.02949378]\n",
            " [ 0.33245804]\n",
            " [ 0.09486441]\n",
            " [-0.02297236]\n",
            " [ 0.11529048]\n",
            " [ 0.36663299]\n",
            " [-0.73866132]\n",
            " [-0.75051342]\n",
            " [-0.69565822]\n",
            " [-0.75914626]\n",
            " [-0.45478114]\n",
            " [-0.18335758]\n",
            " [-0.51417315]\n",
            " [-0.64201462]\n",
            " [-0.46375095]\n",
            " [-0.13271705]\n",
            " [ 0.        ]]\n",
            "objective value: [0.2408722242249906, 0.20779160655816206, 0.18668164288018457, 0.17189564345509498, 0.1609036195966798, 0.15235883011267373, 0.14548215993650115, 0.13979729343412772, 0.1349977979936765, 0.1308771454840259, 0.1272904923095636, 0.12413283723386742, 0.1213259382647701, 0.1188101304143519, 0.11653901204792608, 0.11447588059289301, 0.11259127419420122, 0.11086123479184497, 0.10926605479642519, 0.10778935577387112, 0.10641739992229038, 0.10513856786219239, 0.10394295725791225, 0.10282207056567493, 0.10176856943179749, 0.10077607956086378, 0.09983903424251817, 0.09895254780371297, 0.0981123124527668, 0.09731451357371262, 0.09655575969572208, 0.0958330242262848, 0.09514359668333819, 0.09448504165005285, 0.09385516404842861, 0.09325197961422098, 0.09267368967764425, 0.09211865952756952, 0.09158539977317537, 0.09107255022484216, 0.09057886590196979, 0.09010320484421416, 0.08964451745809081, 0.08920183717581277, 0.08877427223980926, 0.08836099845629908, 0.08796125278589485, 0.08757432765952712, 0.0871995659248209, 0.08683635634207917, 0.08648412956074819, 0.0861423545170698, 0.08581053520190343, 0.08548820775469236, 0.08517493784547357, 0.08487031831187064, 0.08457396702230373, 0.08428552494032762, 0.0840046543681596, 0.08373103735017041, 0.08346437421944998, 0.08320438227257924, 0.08295079455949249, 0.0827033587768346, 0.08246183625454383, 0.08222600102654509, 0.08199563897745096, 0.08177054705805235, 0.0815505325631594, 0.08133541246603625, 0.08112501280427831, 0.08091916811251057, 0.08071772089775943, 0.08052052115376605, 0.08032742591088028, 0.08013829881850443, 0.07995300975734854, 0.07977143447902106, 0.07959345427071247, 0.07941895564293848, 0.07924783003849671, 0.07907997356095887, 0.07891528672117108, 0.07875367420037153, 0.07859504462865564, 0.0784393103776308, 0.07828638736620035, 0.07813619487850869, 0.07798865539315875, 0.07784369442288808, 0.07770124036395674, 0.07756122435456, 0.07742358014163517, 0.07728824395548181, 0.07715515439165965, 0.07702425229967157, 0.07689548067797616, 0.07676878457490924, 0.07664411099512602, 0.07652140881120456]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "# I obtained the optimal weights and a list of objective values by using gradient_descent function.\n",
        "n, d = x_train.shape\n",
        "y_train = numpy.array(y_train, dtype=numpy.float32).reshape(-1,1)\n",
        "lr = 0.5 #[0.01, 0.1, 0.5, 1]\n",
        "lambdas = 0.001 #[0.001, 0.05, 0.1, 0.5]\n",
        "\n",
        "# Initialize the weights to zero\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_gd_r, gd_obj_r = gradient_descent(x_train, y_train, lambdas, lr, w)\n",
        "#gd_obj_r = numpy.concatenate(numpy.array(objvals)).ravel()\n",
        "# Print the optimal weights and the last objective value\n",
        "print(\"Optimal weights:\", w_gd_r)\n",
        "print(\"objective value:\", gd_obj_r)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PIJ2frwGpf18"
      },
      "source": [
        "## 3.2. Stochastic gradient descent (SGD)\n",
        "\n",
        "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
        "\n",
        "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "4FsNjQK5pf18"
      },
      "outputs": [],
      "source": [
        "# Calculated the objective Q_i and the gradient of Q_i\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: 1-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def stochastic_objective_gradient(w, xi, yi, lam):\n",
        "    xyw = numpy.multiply(yi, numpy.dot(xi, w))\n",
        "    val1 = numpy.array(xyw, dtype= numpy.float128)\n",
        "    \n",
        "    exp_term = numpy.exp(-val1)\n",
        "    log_loss = numpy.log(1 + exp_term)\n",
        "    reg_term = 0.5 * lam * numpy.linalg.norm(w) ** 2\n",
        "    obj = log_loss + reg_term\n",
        "\n",
        "    term_exp = numpy.exp(val1)\n",
        "    grad =  -numpy.dot(xi.T, yi / (1 + term_exp))\n",
        "    reg = lam * w\n",
        "    g = grad + reg\n",
        "    return obj, g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "vbznMoVspf19"
      },
      "outputs": [],
      "source": [
        "# SGD for solving logistic regression\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     \n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    n, d = x.shape\n",
        "    objvals = []\n",
        "    for epoch in range(max_epoch):\n",
        "        objval = 0\n",
        "        rand_perm = numpy.random.permutation(n) #randomly assigning indices to the iterations that need to compute\n",
        "        for i in rand_perm: \n",
        "            xi = numpy.array(x[i, :]).reshape(1, -1)\n",
        "            yi = y[i]\n",
        "            obj, grad = stochastic_objective_gradient(w, xi, yi, lam)\n",
        "            w = w - learning_rate * grad\n",
        "            xi = numpy.array(x[i, :]).reshape(1, -1)\n",
        "            yi = y[i]\n",
        "            obj, _ = stochastic_objective_gradient(w, xi, yi, lam)\n",
        "            objval += obj\n",
        "        objval /= n \n",
        "        objvals.append(objval)\n",
        "    return w, objvals"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u3qC4se7pf19"
      },
      "source": [
        "Use sgd function to obtain optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQsP8JYpf19",
        "outputId": "fa35ad56-a9a8-4814-bbda-2906f34ddeed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal weights: [[ 0.55902611]\n",
            " [ 0.50230471]\n",
            " [ 0.5486133 ]\n",
            " [ 1.19289203]\n",
            " [-1.66058689]\n",
            " [ 3.07202286]\n",
            " [-2.46722012]\n",
            " [-2.46305344]\n",
            " [ 1.08190107]\n",
            " [-0.19904265]\n",
            " [-4.10190618]\n",
            " [ 0.73320378]\n",
            " [ 0.37782005]\n",
            " [-3.07076005]\n",
            " [-0.97889935]\n",
            " [-0.828874  ]\n",
            " [ 1.67165998]\n",
            " [-1.22947246]\n",
            " [ 1.23170982]\n",
            " [ 2.90504838]\n",
            " [-3.10265524]\n",
            " [-3.98691088]\n",
            " [-2.08351173]\n",
            " [-2.04571989]\n",
            " [ 0.92460425]\n",
            " [ 0.81910613]\n",
            " [-1.77759311]\n",
            " [-2.27352402]\n",
            " [-2.01576507]\n",
            " [-1.30048615]\n",
            " [ 0.        ]]\n",
            "objective value: [0.07568184 0.05484753 0.04941173 0.04826076 0.04874952 0.04638736\n",
            " 0.04371117 0.04414778 0.04283453 0.04039138 0.04256305 0.04008804\n",
            " 0.04496157 0.04013478 0.04006432 0.04076056 0.04104249 0.03919122\n",
            " 0.04011802 0.04225497 0.03825243 0.04050131 0.03908332 0.0384502\n",
            " 0.0376509  0.0401731  0.03936624 0.03773094 0.04133316 0.03775476\n",
            " 0.03464056 0.03791469 0.03923236 0.03835158 0.0376423  0.03669628\n",
            " 0.03692745 0.03540511 0.03574552 0.03309514 0.03718607 0.03654089\n",
            " 0.03575787 0.03632966 0.03331979 0.03523657 0.0358688  0.03535152\n",
            " 0.03734257 0.03369155 0.03323861 0.0375659  0.03279853 0.03532886\n",
            " 0.03547733 0.03189144 0.03455705 0.03526053 0.03371468 0.03619773\n",
            " 0.03362704 0.03589049 0.03462932 0.03348001 0.03362964 0.03397909\n",
            " 0.03263634 0.03436942 0.03256409 0.03161429 0.03282302 0.03340727\n",
            " 0.03023227 0.03414611 0.0318814  0.03200737 0.03496191 0.03260116\n",
            " 0.03197151 0.03142799 0.0326325  0.03194581 0.03221315 0.03232233\n",
            " 0.03200982 0.0315288  0.03171915 0.03288391 0.03256061 0.03044405\n",
            " 0.03527579 0.03230954 0.03291614 0.03250079 0.03242672 0.03438664\n",
            " 0.03171031 0.03265282 0.0307297  0.03122548]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "n, d = x_train.shape\n",
        "y_train = numpy.array(y_train, dtype=numpy.float32).reshape(-1,1)\n",
        "#print(x_train.shape[1])\n",
        "lr = 0.1 #[0.01, 0.1, 0.5, 1]\n",
        "lambdas = 0 #[0.1, 0.5, 1, 5]\n",
        "\n",
        "# Initialize the weights to zero\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_sgd, objvals = sgd(x_train, y_train, lambdas, lr, w)\n",
        "sgd_obj = numpy.concatenate(numpy.array(objvals)).ravel()\n",
        "# Print the optimal weights and the last objective value\n",
        "print(\"Optimal weights:\", w_sgd)\n",
        "print(\"objective value:\", sgd_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXxm9ag2pf19",
        "outputId": "ea3e98d3-f7ae-4a32-fbce-cdfd89768704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal weights: [[-0.30252335]\n",
            " [-0.3088444 ]\n",
            " [-0.28985127]\n",
            " [-0.47246239]\n",
            " [-0.17852478]\n",
            " [ 0.61566113]\n",
            " [-0.97283032]\n",
            " [-1.03763851]\n",
            " [ 0.08245569]\n",
            " [ 0.18505655]\n",
            " [-1.43983515]\n",
            " [ 0.00301809]\n",
            " [-0.86214253]\n",
            " [-1.17944052]\n",
            " [-0.21683551]\n",
            " [ 0.7671238 ]\n",
            " [ 0.06121545]\n",
            " [-0.21755325]\n",
            " [ 0.37278136]\n",
            " [ 0.68565617]\n",
            " [-1.07272462]\n",
            " [-1.4203845 ]\n",
            " [-0.92279297]\n",
            " [-1.13726271]\n",
            " [-0.48579062]\n",
            " [ 0.0260356 ]\n",
            " [-0.90454418]\n",
            " [-1.13154753]\n",
            " [-0.90861114]\n",
            " [-0.3182424 ]\n",
            " [ 0.        ]]\n",
            "objective value: [0.21292801 0.12252728 0.10449364 0.09554037 0.0898096  0.08589448\n",
            " 0.08291038 0.0805488  0.07866522 0.07725341 0.07597334 0.07476683\n",
            " 0.07387997 0.07300593 0.07231883 0.07151309 0.07101234 0.07040659\n",
            " 0.06993796 0.06948034 0.06904629 0.06861221 0.06832023 0.06795481\n",
            " 0.06765758 0.06742288 0.06711862 0.06689985 0.0665691  0.06647088\n",
            " 0.06629625 0.06606907 0.06590174 0.06557943 0.06557871 0.06535025\n",
            " 0.06521317 0.06506568 0.06498831 0.06474356 0.06469231 0.06463068\n",
            " 0.06449763 0.06441602 0.06434085 0.0642146  0.0641202  0.06400933\n",
            " 0.06400922 0.06381478 0.06378057 0.06378703 0.06366711 0.06361386\n",
            " 0.06350623 0.06350543 0.06328299 0.06342324 0.06335205 0.06330155\n",
            " 0.06313284 0.06322966 0.06311444 0.06307308 0.06293461 0.063029\n",
            " 0.06294873 0.06278648 0.0628725  0.06290582 0.06280772 0.0627705\n",
            " 0.06274903 0.06272864 0.06259587 0.06261909 0.06267177 0.06259194\n",
            " 0.06233457 0.06254786 0.06250168 0.06246865 0.06239819 0.06234585\n",
            " 0.06237133 0.06238804 0.06239487 0.0623407  0.06235198 0.0620427\n",
            " 0.06227989 0.06223329 0.06216712 0.06220969 0.06220376 0.06218369\n",
            " 0.06212466 0.06213397 0.06202201 0.06197852]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "n, d = x_train.shape\n",
        "y_train = numpy.array(y_train, dtype=numpy.float32).reshape(-1,1)\n",
        "#print(x_train.shape[1])\n",
        "lr = 0.01 \n",
        "lambdas = 0.001 \n",
        "\n",
        "# Initialize the weights to zero\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_sgd_r, objvals = sgd(x_train, y_train, lambdas, lr, w)\n",
        "sgd_obj_r = numpy.concatenate(numpy.array(objvals)).ravel()\n",
        "# Print the optimal weights and the last objective value\n",
        "print(\"Optimal weights:\", w_sgd_r)\n",
        "print(\"objective value:\", sgd_obj_r)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QrDJDfeQpf19"
      },
      "source": [
        "## 3.3 Mini-Batch Gradient Descent (MBGD)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B_lKzyBSpf19"
      },
      "source": [
        "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "wHMiRddXpf19"
      },
      "outputs": [],
      "source": [
        "# Calculated the objective Q_I and the gradient of Q_I\n",
        "# Inputs:\n",
        "#     w: weights: d-by-b matrix\n",
        "#     xi: data: b-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def mb_objective_gradient(w, xi, yi, lam):\n",
        "    b = 20\n",
        "    xy = numpy.multiply(xi,yi)\n",
        "    xyw = numpy.dot(xy,w)\n",
        "    val1 = numpy.array(xyw, dtype= numpy.float128)\n",
        "\n",
        "    exp_term = (1 + numpy.exp(-val1)).astype(float)\n",
        "    term = numpy.log(exp_term)\n",
        "    log = 1/b * numpy.sum(term)\n",
        "    reg = 0.5 * lam * numpy.linalg.norm(w)**2\n",
        "    obj = log + reg\n",
        "    \n",
        "   # val = numpy.asarray([[dc.Decimal(i) for i in j] for j in xyw])\n",
        "    exp_term = (1 + numpy.exp(val1)).astype(float)\n",
        "   # print(exp_term)\n",
        "    term = numpy.divide(xy,exp_term)\n",
        "    grad = -1/b * numpy.sum(term, axis=0).reshape(-1,1)\n",
        "    reg = lam * w\n",
        "    g = grad + reg\n",
        "    return obj, g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "RtT-j1rLpf19"
      },
      "outputs": [],
      "source": [
        "# MBGD for solving logistic regression\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    n, d = x.shape\n",
        "    objvals = []\n",
        "    b = 20 # mini-batch size\n",
        "    for epoch in range(max_epoch):\n",
        "        num = numpy.random.permutation(n)\n",
        "        for i in range(0, n, b):\n",
        "            indices = num[i:i+b]\n",
        "            xi = x[indices, :]\n",
        "            yi = y[indices, :]\n",
        "            obj, grad = mb_objective_gradient(w, xi, yi, lam)\n",
        "            w = w - learning_rate * grad\n",
        "        objvals.append(obj)\n",
        "    return w, objvals"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kJU02xF0pf19"
      },
      "source": [
        "Use mbgd function to obtain optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9KqVeK3pf19",
        "outputId": "32354bea-17a8-4f4c-8d7c-748b6a7fa467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal weights: [[-0.41776375]\n",
            " [-0.45530537]\n",
            " [-0.40273368]\n",
            " [-0.58843731]\n",
            " [-0.16494862]\n",
            " [ 0.47577669]\n",
            " [-0.93472109]\n",
            " [-0.96763843]\n",
            " [ 0.00379494]\n",
            " [ 0.26139033]\n",
            " [-1.3398504 ]\n",
            " [-0.06931189]\n",
            " [-0.90768145]\n",
            " [-1.1096505 ]\n",
            " [-0.1568935 ]\n",
            " [ 0.81093291]\n",
            " [ 0.01045496]\n",
            " [-0.13222687]\n",
            " [ 0.33084919]\n",
            " [ 0.66711622]\n",
            " [-1.0470305 ]\n",
            " [-1.27470733]\n",
            " [-0.92872702]\n",
            " [-1.12705057]\n",
            " [-0.63607001]\n",
            " [-0.00854979]\n",
            " [-0.90448877]\n",
            " [-1.0638366 ]\n",
            " [-0.82992406]\n",
            " [-0.27101535]\n",
            " [ 0.        ]]\n",
            "Objective value: [0.1027799674664659, 0.11210857960977788, 0.04784902693423598, 0.07104561123777782, 0.17071092873785154, 0.033975144233800235, 0.048538616328305695, 0.15781672397093716, 0.020041133185400833, 0.05980620686813686, 0.029225362699316606, 0.24546034225777147, 0.04823822521835777, 0.007718186177558287, 0.08883181666482903, 0.055389662966584444, 0.048645452519751886, 0.2470136875967558, 0.00986679224022519, 0.1805433661652762, 0.03243733550902566, 0.04300810563194036, 0.016985266665983496, 0.02978070261674097, 0.02893934381190932, 0.04303766058977018, 0.12044071248620974, 0.1562492202729172, 0.1231548021793974, 0.025423397831131178, 0.014271489023569693, 0.03806985656272055, 0.005621305076822875, 0.006887465611839926, 0.015915665161374517, 0.018245573130880904, 0.01847228789011101, 0.014485471873508524, 0.02317758349816206, 0.08326521244635413, 0.008332988774830847, 0.01339246679898324, 0.06475918351212467, 0.014346610640428817, 0.012925464938016551, 0.011028668345667222, 0.03339813731868578, 0.057156841575479615, 0.11576500339421039, 0.20286100847370292, 0.022377378671618615, 0.02227690467376544, 0.01500929506318846, 0.10347330664566662, 0.0027769608661746092, 0.010209794863375844, 0.018528978320502622, 0.12006618767797553, 0.03971866749168895, 0.009342996131965027, 0.023491165415163707, 0.11880144548489585, 0.017778582031031106, 0.030785835273304626, 0.03408876369541406, 0.013703341644194556, 0.03387956655849222, 0.23359601957151557, 0.014232125705462726, 0.059230983563327566, 0.01779139193964919, 0.2037888079678757, 0.10093002058836863, 0.12329691566243475, 0.01628481805591961, 0.0030855500584453174, 0.3514505314695002, 0.00849614297745325, 0.06690589556352994, 0.008611309618991545, 0.01711781592269124, 0.03661757882983736, 0.13028740263708194, 0.046209210620562685, 0.05779879504016733, 0.054824986864741, 0.029864427260284893, 0.04238646902887506, 0.005967161704016237, 0.05095423747878724, 0.020093370697750114, 0.04677068820545452, 0.025438191978480063, 0.010578978699740308, 0.2905789348671213, 0.021251237766399995, 0.007341756026102372, 0.025954250803669723, 0.031598358620288806, 0.016682241417529282]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "n, d = x_train.shape\n",
        "\n",
        "lr = 0.1 #[0.01, 0.1, 0.5, 1]\n",
        "lambdas = 0 #[0.1, 0.5, 1, 5]\n",
        "\n",
        "# Initialize the weights to zero\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_mbgd, obj_mbgd = mbgd(x_train, y_train, lambdas, lr, w)\n",
        "\n",
        "# Print the optimal weights and the objective value\n",
        "print(\"Optimal weights:\", w_mbgd)\n",
        "print(\"Objective value:\", obj_mbgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ynapNQpf19",
        "outputId": "1b24e585-93af-4d32-e535-42cf95f4ccee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal weights: [[-0.40395985]\n",
            " [-0.42937428]\n",
            " [-0.39098602]\n",
            " [-0.55776425]\n",
            " [-0.14293561]\n",
            " [ 0.41516127]\n",
            " [-0.85936557]\n",
            " [-0.89152788]\n",
            " [-0.00772605]\n",
            " [ 0.24496414]\n",
            " [-1.22147523]\n",
            " [-0.06142274]\n",
            " [-0.83841499]\n",
            " [-1.01630243]\n",
            " [-0.1410143 ]\n",
            " [ 0.72566846]\n",
            " [ 0.01966294]\n",
            " [-0.11759654]\n",
            " [ 0.28935031]\n",
            " [ 0.58904063]\n",
            " [-0.96903919]\n",
            " [-1.16279784]\n",
            " [-0.86499923]\n",
            " [-1.04158076]\n",
            " [-0.58130435]\n",
            " [-0.02076253]\n",
            " [-0.82273237]\n",
            " [-0.97486594]\n",
            " [-0.75718717]\n",
            " [-0.24639164]\n",
            " [ 0.        ]]\n",
            "objective value: [0.14569960807456342, 0.13451092192016018, 0.045350562996819846, 0.0722111504355657, 0.11491871027353895, 0.09579823344927324, 0.11384057892032179, 0.043709894700844326, 0.06682742294602514, 0.08543514699738364, 0.1564623466468393, 0.03453176737234619, 0.1734661425879582, 0.08714047152805696, 0.02974971987705699, 0.22401853978662223, 0.05963148086394393, 0.028371513722022658, 0.017011607156278422, 0.0254708733940604, 0.04036687312751138, 0.039148351483406105, 0.27933476878172275, 0.08555618428082783, 0.04645637810141175, 0.10740148073634313, 0.04037784527979915, 0.11387942754198518, 0.03702247868097631, 0.06800250943248355, 0.1479136478354749, 0.022840455140883537, 0.017696744696609725, 0.04693686133279723, 0.028338506055137005, 0.029320141186924303, 0.02365269484362658, 0.02214862573365677, 0.021805753717536933, 0.036405985466593314, 0.031661505398839246, 0.03815770374693904, 0.07160159899999578, 0.018972676669456026, 0.017264109674159433, 0.05914320972661888, 0.053643921953917116, 0.017058145192428334, 0.011718390440195068, 0.12281142109195697, 0.06676293709528376, 0.0280740881244205, 0.010403286424243818, 0.011379749661184758, 0.016048967840002412, 0.022955850197746118, 0.14305378777773806, 0.02023966763137607, 0.061838230092489364, 0.1044932539869934, 0.01960331687527964, 0.03096551497065804, 0.028095055059246504, 0.03672353552115316, 0.01241318656459933, 0.020285479799441834, 0.05777338974985922, 0.013196923413969055, 0.010114125323571175, 0.03224575224092679, 0.02831858785627133, 0.01916695734258618, 0.09427090334397428, 0.06225735418340575, 0.026561089655157164, 0.015022612304655573, 0.0921300869923159, 0.0476736419200344, 0.028492648046444337, 0.013472614232876403, 0.03956909602320209, 0.016656821350459712, 0.014339983603756563, 0.0224059788211911, 0.11501955268240213, 0.07906775301419983, 0.02210278503139341, 0.15061038981994787, 0.029339832033702638, 0.023185567180745843, 0.058563413250758325, 0.03963578334563098, 0.04692424139665141, 0.03264023951907511, 0.030707573381122052, 0.0768196419175029, 0.02133753700477244, 0.04577044325158515, 0.023755679647307855, 0.03149308144397262]\n"
          ]
        }
      ],
      "source": [
        "# Trained regularized logistic regression\n",
        "n, d = x_train.shape\n",
        "y_train = numpy.array(y_train, dtype=numpy.float32).reshape(-1,1)\n",
        "#print(x_train.shape[1])\n",
        "lr = 0.1 #[0.01, 0.1, 0.5, 1]\n",
        "lambdas = 0.001 #[0.1, 0.5, 1, 5]\n",
        "\n",
        "# Initialize the weights to zero\n",
        "w = numpy.zeros((d, 1))\n",
        "\n",
        "w_mbgd_r, obj_mbgd_r = mbgd(x_train, y_train, lambdas, lr, w)\n",
        "\n",
        "# Print the optimal weights and the last objective value\n",
        "print(\"Optimal weights:\", w_mbgd_r)\n",
        "print(\"objective value:\", obj_mbgd_r)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dt9Aq2Y4pf19"
      },
      "source": [
        "# 4. Compare GD, SGD, MBGD\n",
        "\n",
        "### Plot objective function values against epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "xjVHvR1Ipf19",
        "outputId": "16dfca68-6c83-411c-c826-11f250c29017"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
        "\n",
        "# Plot the first function on the first subplot\n",
        "axes[1].plot(sgd_obj, label='Stochastic Gradient Descent', linestyle = 'dotted', marker='.' ,color = 'lightblue')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Objective Values')\n",
        "axes[1].set_ylim(bottom=0, top=0.3)\n",
        "\n",
        "# Plot the second function on the second subplot\n",
        "axes[0].plot(gd_obj, label='Gradient Descent', linestyle = 'dotted',marker = '.', color='red')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Objective values')\n",
        "axes[0].set_ylim(bottom=0, top=0.5)\n",
        "\n",
        "axes[2].plot(obj_mbgd, label='Mini Batch Gradient Descent', linestyle = 'dotted',marker = '.',color = 'pink')\n",
        "axes[2].set_xlabel('Epochs')\n",
        "axes[2].set_ylabel('Objective Values')\n",
        "axes[2].set_ylim(bottom=0, top=5)\n",
        "\n",
        "# Adjust spacing between subplots and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qLmsmwicpf19"
      },
      "source": [
        "# 5. Prediction\n",
        "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "ol-sorL5pf19"
      },
      "outputs": [],
      "source": [
        "# Predict class label\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     X: data: m-by-d matrix\n",
        "# Return:\n",
        "#     f: m-by-1 matrix, the predictions\n",
        "def predict(w, x):\n",
        "    f = numpy.dot(x, w)\n",
        "    return numpy.sign(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z_LoU5Hpf19",
        "outputId": "84e37682-cfa2-4055-b335-bffa2cb5e802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error rate of Logistic Regression for Stochastic Gradient Descent: 0.01\n",
            "Accuracy of Logistic Regression for Stochastic Gradient Descent: 0.99 \n",
            "\n",
            "Error rate of Regularized Logistic Regression for Stochastic Gradient Descent: 0.02\n",
            "Accuracy of Regularized Logistic Regression for Stochastic Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Logistic Regression for Minibatch Gradient Descent: 0.02\n",
            "Accuracy of Logistic Regression for Minibatch Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Regularized Logistic Regression for Minibatch Gradient Descent: 0.02\n",
            "Accuracy of Regularized Logistic Regression for Minibatch Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Logistic Regression for normal Gradient Descent: 0.02\n",
            "Accuracy of Logistic Regression for normal Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Regularized Logistic Regression for normal Gradient Descent: 0.02\n",
            "Accuracy of Regularized Logistic Regression for normal Gradient Descent: 0.98 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate training error of logistic regression and regularized version\n",
        "arr = {\n",
        "  \"Stochastic\": w_sgd,\n",
        "  \"Stochastic_r\": w_sgd_r ,\n",
        "  \"Minibatch\": w_mbgd,\n",
        "  \"Minibatch_r\": w_mbgd_r, \n",
        "  \"normal\" : w_gd,\n",
        "  \"normal_r\" : w_gd_r\n",
        "}\n",
        "\n",
        "def train_error(arr, x_data, y_data):\n",
        "    n = x_data.shape[0]\n",
        "    for key ,val in arr.items():\n",
        "        if(str(key[-1]) == 'r'):\n",
        "            pred_r = predict(val, x_data)\n",
        "            incorrect_r = numpy.sum(pred_r != y_data)\n",
        "            error_rate = incorrect_r/n\n",
        "            accuracy = 1-error_rate\n",
        "            print(\"Error rate of Regularized Logistic Regression for\",key[:-2],\"Gradient Descent:\",round(error_rate, 2))\n",
        "            print(\"Accuracy of Regularized Logistic Regression for\",key[:-2],\"Gradient Descent:\",round(accuracy, 2),\"\\n\")\n",
        "        else:\n",
        "            pred = predict(val, x_data)\n",
        "            incorrect = numpy.sum(pred != y_data)\n",
        "            error_rate = incorrect/n\n",
        "            accuracy = 1-error_rate\n",
        "            print(\"Error rate of Logistic Regression for\",key,\"Gradient Descent:\",round(error_rate, 2))\n",
        "            print(\"Accuracy of Logistic Regression for\",key,\"Gradient Descent:\",round(accuracy, 2),\"\\n\")\n",
        "\n",
        "train_error(arr, x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3niaaXIpf19",
        "outputId": "edd319f8-183a-40db-8deb-fab37b5f5282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error rate of Logistic Regression for Stochastic Gradient Descent: 0.04\n",
            "Accuracy of Logistic Regression for Stochastic Gradient Descent: 0.96 \n",
            "\n",
            "Error rate of Regularized Logistic Regression for Stochastic Gradient Descent: 0.02\n",
            "Accuracy of Regularized Logistic Regression for Stochastic Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Logistic Regression for Minibatch Gradient Descent: 0.02\n",
            "Accuracy of Logistic Regression for Minibatch Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Regularized Logistic Regression for Minibatch Gradient Descent: 0.02\n",
            "Accuracy of Regularized Logistic Regression for Minibatch Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Logistic Regression for normal Gradient Descent: 0.02\n",
            "Accuracy of Logistic Regression for normal Gradient Descent: 0.98 \n",
            "\n",
            "Error rate of Regularized Logistic Regression for normal Gradient Descent: 0.02\n",
            "Accuracy of Regularized Logistic Regression for normal Gradient Descent: 0.98 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate testing error of logistic regression and regularized version\n",
        "y_test = numpy.array(y_test, dtype=numpy.float32).reshape(-1,1)\n",
        "def train_error(arr, x_data, y_data):\n",
        "    n = x_data.shape[0]\n",
        "    for key ,val in arr.items():\n",
        "        if(str(key[-1]) == 'r'):\n",
        "            pred_r = predict(val, x_data)\n",
        "            incorrect_r = numpy.sum(pred_r != y_data)\n",
        "            error_rate = incorrect_r/n\n",
        "            accuracy = 1-error_rate\n",
        "            print(\"Error rate of Regularized Logistic Regression for\",key[:-2],\"Gradient Descent:\",round(error_rate, 2))\n",
        "            print(\"Accuracy of Regularized Logistic Regression for\",key[:-2],\"Gradient Descent:\",round(accuracy, 2),\"\\n\")\n",
        "        else:\n",
        "            pred = predict(val, x_data)\n",
        "            incorrect = numpy.sum(pred != y_data)\n",
        "            error_rate = incorrect/n\n",
        "            accuracy = 1-error_rate\n",
        "            print(\"Error rate of Logistic Regression for\",key,\"Gradient Descent:\",round(error_rate, 2))\n",
        "            print(\"Accuracy of Logistic Regression for\",key,\"Gradient Descent:\",round(accuracy, 2),\"\\n\")\n",
        "\n",
        "train_error(arr, x_test, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cBXMMKSnpf19"
      },
      "source": [
        "# 6. Parameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9JUxpi9OZSu",
        "outputId": "2f669afc-169a-448b-cde6-22e3ee1e3d61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-72-b019f2a65a89>:17: RuntimeWarning: overflow encountered in exp\n",
            "  exp_term = (1 + numpy.exp(-val1)).astype(float)\n",
            "<ipython-input-72-b019f2a65a89>:24: RuntimeWarning: overflow encountered in exp\n",
            "  exp_term = (1 + numpy.exp(val1)).astype(float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged test errors(validation error) 0.01\n",
            "Average test accuracy(validation error) 0.99\n"
          ]
        }
      ],
      "source": [
        "from functools import reduce\n",
        "\n",
        "# Define a function that performs the training and evaluation on a single fold\n",
        "def cross_val_kfold(i,X_parts,y_parts, lam, lr):    \n",
        "    x_test = X_parts[i]\n",
        "    y_test = y_parts[i]\n",
        "    # Set the remaining parts as the training data\n",
        "    x_train = numpy.concatenate(X_parts[:i] + X_parts[i+1:])\n",
        "    y_train = numpy.concatenate(y_parts[:i] + y_parts[i+1:])\n",
        "    d = x_train.shape[1]\n",
        "    w = numpy.zeros((d,1))\n",
        "\n",
        "    #change the name of function if you need mini batch write 'mbgd' and for stochastic 'sgd'\n",
        "    w_, obj_ = mbgd(x_train, y_train, lam, lr, w)     \n",
        "    # Make predictions on the test set and compute the error rate and accuracy\n",
        "    pred = predict(w_, x_test)\n",
        "    incorrect_r = numpy.sum(pred != y_test)\n",
        "    error_rate = incorrect_r / n\n",
        "    accuracy = 1 - error_rate\n",
        "    return (error_rate, accuracy)\n",
        "\n",
        "# Define the number of folds \n",
        "k = 20\n",
        "avg_rate, avg_acc = 0, 0\n",
        "X = data1[:,1:32]\n",
        "y = (data1[:,0]).reshape(-1,1)\n",
        "X_parts = numpy.array_split(X, k)\n",
        "y_parts = numpy.array_split(y, k)\n",
        "# Use map and filter to apply the train_eval_fold function to each fold\n",
        "output = map(lambda i: cross_val_kfold(i,X_parts,y_parts, 0.01, 0.1), range(k))\n",
        "# Use reduce to compute the running sum of error rates and accuracies\n",
        "avg_error_rate, avg_accuracy = reduce(lambda acc, res: (acc[0] + res[0], acc[1] + res[1]), output, (0, 0))\n",
        "# Compute the average error rate and accuracy over all folds\n",
        "avg_error_rate /= k\n",
        "avg_accuracy /= k\n",
        "print(\"Averaged test errors(validation error)\",round(avg_error_rate,2))\n",
        "print(\"Average test accuracy(validation error)\",round(avg_accuracy,2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0RAaWPybZ2eg"
      },
      "source": [
        "The above, **Tuning parameter** includes a function which consists logic for cross **validation kfold** log where I have divided x and y training amd testing set after fetching split from the actual data. After that, I have called **all three gradient functions** to obtain **updated weights**. This weights helps to **predict** the **validation set**  **error rate and accuray** per functions, which is then **averaged** for obtaining **validation error**. However, This function takes **longer time to compute** if loops are used. Hence, to avoid that issue, I have used optimized way by inculcating **map and reduce** concepts. I reduced time complexity by 4 times as before (while using normal for loop) it took 40.06 mins (more than half an hour)to compute the acccuracy and error rate, but now it takes only in 10 to 15 secs. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
